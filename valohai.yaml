- step:
    name: data-preprocess
    image: valohai/llm-toolkit:0.1
    environment: trial2023-g4dn-xlarge
    command:
      - pip install -r requirements.in
      - python data-preprocess.py {parameters}
    parameters:
      - name: tokenizer
        type: string
        default: 'mistralai/Mistral-7B-v0.1'
      - name: model_max_length
        type: integer
        default: 512
    inputs:
      - name: dataset
        default:
        - s3://dd-sample-bucket/mistral/gem-viggo-dataset/test.csv
        - s3://dd-sample-bucket/mistral/gem-viggo-dataset/train.csv
        - s3://dd-sample-bucket/mistral/gem-viggo-dataset/validation.csv

- step:
    name: finetune
    image: valohai/llm-toolkit:0.1
    environment: trial2023-g4dn-xlarge
    command:
      - pip install -r requirements.in
      - python finetune-mistral.py {parameters}
    parameters:
      - name: base_mistral_model
        default: "mistralai/Mistral-7B-v0.1"
      - name: output_dir
        type: string
        default: "finetuned_mistral"
      - name: model_max_length
        type: integer
        default: 512
      - name: warmup_steps
        type: integer
        default: 5
      - name: max_steps
        type: integer
        default: 36
      - name: learning_rate
        type: float
        default: 2.5e-5
      - name: do_eval
        type: flag
        default: False
    inputs:
      - name: train_data
        default: dataset://viggo/dev_train
      - name: test_data
        default: dataset://viggo/dev_test
      - name: val_data
        default: dataset://viggo/dev_val

- step:
    name: inference
    image: valohai/llm-toolkit:0.1
    environment: trial2023-g4dn-xlarge
    command:
      - pip install -r requirements.in
      - python inference-mistral.py {parameters}
    parameters:
      - name: base_mistral_model
        default: "mistralai/Mistral-7B-v0.1"
      - name: max_tokens
        type: integer
        default: 500
      - name: prompt
        type: string
        default: "
        Given a meaning representation generate a target sentence that utilizes the attributes and attribute values given. The sentence should use all the information provided in the meaning representation. Below is an example pair. Complete the second pair.
        ### Meaning representation:
        inform(name[Need for Speed: Payback], release_year[2017], genres[driving/racing], player_perspective[third person], has_multiplayer[yes], platforms[PlayStation, Xbox, PC], has_linux_release[no], has_mac_release[no])
        
        ### Target sentence: 
        Need for Speed: Payback is a third person driving/racing game with a multiplayer mode. It came out in 2017 for PlayStation, Xbox, and PC, but it is not available on Mac or Linux.
        
        ### Meaning representation:
        inform(name[Made Up Game], release_year[2023], rating[good], genres[arcade, puzzle], has_multiplayer[no], available_on_steam[yes])

        ### Target sentence:
        "
    inputs:
      - name: finetuned-checkpoint
        default: dataset://mistral-models/best_mistral_checkpoint

- pipeline:
    name: training-pipeline
    nodes:
      - name: preprocess
        type: execution
        step: data-preprocess
      - name: train
        type: execution
        step: finetune
      - name: inference
        type: execution
        step: inference
    edges:
      - [preprocess.output.encoded_val/*, train.input.val_data]
      - [preprocess.output.encoded_train/*, train.input.train_data]
      - [preprocess.output.encoded_test/*, train.input.test_data]
      - [train.output.finetuned_mistral.best_model/*, inference.input.finetuned-checkpoint]
      - [train.parameter.base_mistral_model, inference.parameter.base_mistral_model]

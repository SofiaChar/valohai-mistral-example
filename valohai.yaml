- step:
    name: data-preprocess
    image: sofiiavalohai/llm-toolkit:v1.0
    environment: staging-aws-eu-west-1-p3-2xlarge
    command:
      - pip install -r requirements.in
      - python data-preprocess.py {parameters}
    parameters:
      - name: tokenizer
        type: string
        default: 'mistralai/Mistral-7B-v0.1'
      - name: model_max_length
        type: integer
        default: 512
    inputs:
      - name: dataset
        default:
        - s3://dd-sample-bucket/mistral/gem-viggo-dataset/viggo.py
        - s3://dd-sample-bucket/mistral/gem-viggo-dataset/test.csv
        - s3://dd-sample-bucket/mistral/gem-viggo-dataset/train.csv
        - s3://dd-sample-bucket/mistral/gem-viggo-dataset/validation.csv

- step:
    name: finetune
    image: sofiiavalohai/llm-toolkit:v1.0
    environment: staging-aws-eu-west-1-p3-2xlarge
    command:
      - pip install -r requirements.in
      - python finetune-mistral.py {parameters}
    parameters:
      - name: base_mistral_model
        default: "mistralai/Mistral-7B-v0.1"
      - name: output_dir
        type: string
        default: "finetuned_mistral"
      - name: model_max_length
        type: integer
        default: 512
      - name: warmup_steps
        type: integer
        default: 5
      - name: max_steps
        type: integer
        default: 15
      - name: learning_rate
        type: float
        default: 2.5e-5
      - name: do_eval
        type: flag
        default: False
    inputs:
      - name: train_data
        default: dataset://viggo/dev_train
      - name: test_data
        default: dataset://viggo/dev_test
      - name: val_data
        default: dataset://viggo/dev_val

- step:
    name: inference
    image: sofiiavalohai/llm-toolkit:v1.0
    environment: staging-aws-eu-west-1-p3-2xlarge
    command:
      - pip install -r requirements.in
      - python inference-mistral.py {parameters}
    parameters:
      - name: base_mistral_model
        default: "mistralai/Mistral-7B-v0.1"
      - name: prompt
        type: string
        default: "give_opinion(name[SpellForce 3], rating[poor], genres[real-time strategy, role-playing], player_perspective[bird view])"
      - name: max_tokens
        type: integer
        default: 305
    inputs:
      - name: finetuned-checkpoint
        default: dataset://mistral-models/best_mistral_checkpoint
      - name: test_data
        default: dataset://viggo/dev_test

- pipeline:
    name: training-pipeline
    nodes:
      - name: preprocess
        type: execution
        step: data-preprocess
      - name: train
        type: execution
        step: finetune
      - name: inference
        type: execution
        step: inference
    edges:
      - [preprocess.output.encoded_val/*, train.input.val_data]
      - [preprocess.output.encoded_train/*, train.input.train_data]
      - [preprocess.output.encoded_test/*, train.input.test_data]
      - [train.output.finetuned_mistral.best_model/*, inference.input.finetuned-checkpoint]
      - [train.parameter.base_mistral_model, inference.parameter.base_mistral_model]


